import os
import yaml
import argparse
from gptqmodel import GPTQModel, QuantizeConfig
from datasets import load_dataset
import torch

def load_config(config_path):
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_id", type=str, required=True, 
                       help="Hugging Face æ¨¡å‹ ID (å¦‚ 'Qwen/Qwen2.5-32B-Instruct')")
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--config", type=str, default="configs/qwen_32b_gptq.yaml")
    parser.add_argument("--cache_dir", type=str, default="/tmp/hf_cache",
                       help="æ¨¡å‹ç¼“å­˜ç›®å½•")
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    quant_config = QuantizeConfig(**config['quantization'])
    
    # åŠ è½½æ ¡å‡†æ•°æ®
    dataset_cfg = config['calibration_dataset']
    calibration_data = load_dataset(
        dataset_cfg['name'],
        dataset_cfg.get('config'),
        split=dataset_cfg['split'],
        cache_dir=os.getenv("HF_HOME", "/tmp/hf_cache")
    )[dataset_cfg['text_column']][:dataset_cfg['samples']]
    
    # åŠ è½½æ¨¡å‹ - ç›´æ¥ä½¿ç”¨è¿œç¨‹ ID
    print(f"ğŸš€ ä» Hugging Face åŠ è½½æ¨¡å‹: {args.model_id}")
    model = GPTQModel.load(
        args.model_id,
        quant_config,
        torch_dtype=torch.bfloat16,
        cache_dir=args.cache_dir  # æŒ‡å®šç¼“å­˜ä½ç½®
    )
    
    # é‡åŒ–
    model.quantize(
        calibration_data,
        batch_size=config['batch_size'],
        auto_gc=config['auto_gc'],
        buffered_fwd=config['buffered_fwd']
    )
    
    # ä¿å­˜é‡åŒ–æ¨¡å‹
    model.save(args.output_dir)
    
    print(f"âœ… é‡åŒ–æ¨¡å‹å·²ä¿å­˜åˆ°: {args.output_dir}")

if __name__ == "__main__":
    main()
