import argparse
import os
import yaml
import time
from gptqmodel import GPTQModel, QuantizeConfig
from datasets import load_dataset
import torch
from torch.utils.data import DataLoader

def load_config(config_path):
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def main():
    parser = argparse.ArgumentParser(description='Qwen GPTQ é‡åŒ–å·¥å…·')
    parser.add_argument('--model_id', type=str, required=True,
                        help='Hugging Face æ¨¡å‹ IDï¼Œå¦‚ Qwen/Qwen2.5-32B-Instruct')
    parser.add_argument('--output_dir', type=str, required=True,
                        help='ä¿å­˜é‡åŒ–æ¨¡å‹çš„ç›®å½•')
    parser.add_argument('--config', type=str, default='configs/qwen_32b_gptq.yaml',
                        help='YAML é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--cache_dir', type=str, default='/tmp/hf_cache',
                        help='Hugging Face ç¼“å­˜ç›®å½•')
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)

    print(f"âš™ï¸ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    config = load_config(args.config)

    quant_config = QuantizeConfig(**config.get('quantization', {}))

    # âœ… åŠ è½½æ¨¡å‹
    print(f"ğŸš€ åŠ è½½æ¨¡å‹: {args.model_id}")
    model = GPTQModel.load(
        args.model_id,
        quant_config,
        torch_dtype=torch.bfloat16,
        cache_dir=args.cache_dir
    )

    # âœ… åŠ è½½æ ¡å‡†æ•°æ®é›†ï¼ˆå¯é€‰ï¼‰
    dataset_cfg = config.get("calibration_dataset", {})
    if dataset_cfg:
        print(f"ğŸ“Š åŠ è½½æ ¡å‡†æ•°æ®é›†: {dataset_cfg.get('name', 'wikitext')}")
        dataset = load_dataset(
            dataset_cfg.get('name', 'wikitext'),
            dataset_cfg.get('config', 'wikitext-2-v1'),
            split=dataset_cfg.get('split', 'train'),
            cache_dir=args.cache_dir
        )
        text_column = dataset_cfg.get('text_column', 'text')
        samples = dataset_cfg.get('samples', 1024)
        texts = dataset[text_column][:samples]
        batch_size = config.get('batch_size', 4)
        dataloader = DataLoader(texts, batch_size=batch_size)
        model.collect_calibration_data(dataloader)
    else:
        print("âš ï¸ æœªæä¾› calibration_dataset å­—æ®µï¼Œå°†è·³è¿‡ collect_calibration_data() æ­¥éª¤ã€‚")

    # âœ… é‡åŒ–
    print("ğŸ”§ å¼€å§‹é‡åŒ–...")
    if dataset_cfg:
        model.quantize(
            calibration_dataset=dataloader,
            auto_gc=config.get('auto_gc', False),
            buffered_fwd=config.get('buffered_fwd', True)
        )
    else:
        raise ValueError("âŒ ç¼ºå°‘æ ¡å‡†æ•°æ®é›† calibration_datasetï¼Œæ— æ³•è¿›è¡Œé‡åŒ–ï¼")

    # âœ… ä¿å­˜æ¨¡å‹
    print(f"ğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹åˆ°: {args.output_dir}")
    model.save(args.output_dir)

    with open(os.path.join(args.output_dir, "COMPLETED"), "w") as f:
        f.write(f"success|model:{args.model_id}|time:{time.time()}")

    print("âœ… æ‰€æœ‰æ“ä½œå®Œæˆ!")

if __name__ == "__main__":
    main()
